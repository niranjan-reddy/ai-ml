{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Monitor the Environment in Azure Machine Learning (Data Drift)\n",
        "\n",
        "In this hands-on lab scenario you are a Data Scientist for Awesome Company. Recently the company has been using Azure Machine Learning Service to implement and run their machine learning workloads concerning diabetes research. With these services now being deployed to production, there is a need to monitor both the model telemetry and data drift.\n",
        "\n",
        "To accomplish your goal, the following should be completed:\n",
        "* Use Azure Application Insights to monitor activity for a model service endpoint\n",
        "* **Configure data drift monitoring for a dataset**\n",
        "\n",
        "*This example notebook is adopted from openly available Microsoft Learn material.*"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the azureml-datadrift package is installed"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show azureml-datadrift"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to your workspace"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "\n",
        "# Load the workspace from the saved config file\n",
        "ws = Workspace.from_config()\n",
        "print('Ready to work with', ws.name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1621970050567
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a baseline dataset"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Datastore, Dataset\n",
        "\n",
        "\n",
        "# Upload the baseline data\n",
        "default_ds = ws.get_default_datastore()\n",
        "default_ds.upload_files(files=['./data/diabetes.csv', './data/diabetes2.csv'],\n",
        "                       target_path='diabetes-baseline',\n",
        "                       overwrite=True, \n",
        "                       show_progress=True)\n",
        "\n",
        "# Create and register the baseline dataset\n",
        "print('Registering baseline dataset...')\n",
        "baseline_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'diabetes-baseline/*.csv'))\n",
        "baseline_data_set = baseline_data_set.register(workspace=ws, \n",
        "                           name='diabetes baseline',\n",
        "                           description='diabetes baseline data',\n",
        "                           tags = {'format':'CSV'},\n",
        "                           create_new_version=True)\n",
        "\n",
        "print('Baseline dataset registered!')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1621970061564
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a target dataset"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime as dt\n",
        "import pandas as pd\n",
        "\n",
        "print('Generating simulated data...')\n",
        "\n",
        "# Load the smaller of the two data files\n",
        "data = pd.read_csv('data/diabetes2.csv')\n",
        "\n",
        "# We'll generate data for the past 6 weeks\n",
        "weeknos = reversed(range(6))\n",
        "\n",
        "file_paths = []\n",
        "for weekno in weeknos:\n",
        "    \n",
        "    # Get the date X weeks ago\n",
        "    data_date = dt.date.today() - dt.timedelta(weeks=weekno)\n",
        "    \n",
        "    # Modify data to ceate some drift\n",
        "    data['Pregnancies'] = data['Pregnancies'] + 1\n",
        "    data['Age'] = round(data['Age'] * 1.2).astype(int)\n",
        "    data['BMI'] = data['BMI'] * 1.1\n",
        "    \n",
        "    # Save the file with the date encoded in the filename\n",
        "    file_path = 'data/diabetes_{}.csv'.format(data_date.strftime(\"%Y-%m-%d\"))\n",
        "    data.to_csv(file_path)\n",
        "    file_paths.append(file_path)\n",
        "\n",
        "# Upload the files\n",
        "path_on_datastore = 'diabetes-target'\n",
        "default_ds.upload_files(files=file_paths,\n",
        "                       target_path=path_on_datastore,\n",
        "                       overwrite=True,\n",
        "                       show_progress=True)\n",
        "\n",
        "# Use the folder partition format to define a dataset with a 'date' timestamp column\n",
        "partition_format = path_on_datastore + '/diabetes_{date:yyyy-MM-dd}.csv'\n",
        "target_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, path_on_datastore + '/*.csv'),\n",
        "                                                       partition_format=partition_format)\n",
        "\n",
        "# Register the target dataset\n",
        "print('Registering target dataset...')\n",
        "target_data_set = target_data_set.with_timestamp_columns('date').register(workspace=ws,\n",
        "                                                                          name='diabetes target',\n",
        "                                                                          description='diabetes target data',\n",
        "                                                                          tags = {'format':'CSV'},\n",
        "                                                                          create_new_version=True)\n",
        "\n",
        "print('Target dataset registered!')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1621970078472
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assign the compute target"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "cluster_name = \"ac-aml-cluster\"\n",
        "\n",
        "try:\n",
        "    # Check for existing compute target\n",
        "    training_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
        "    print('Found existing cluster, use it.')\n",
        "except ComputeTargetException:\n",
        "    # If it doesn't already exist, say so\n",
        "    print('There is no existing cluster by that name.')\n",
        "    "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1621970182302
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the data drift monitor"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.datadrift import DataDriftDetector\n",
        "\n",
        "# set up feature list\n",
        "features = ['Pregnancies', 'Age', 'BMI']\n",
        "\n",
        "# set up data drift detector\n",
        "monitor = DataDriftDetector.create_from_datasets(ws, 'mslearn-diabates-drift', baseline_data_set, target_data_set,\n",
        "                                                      compute_target=cluster_name, \n",
        "                                                      frequency='Week', \n",
        "                                                      feature_list=features, \n",
        "                                                      drift_threshold=.3, \n",
        "                                                      latency=24)\n",
        "monitor"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1621970213957
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backfill the data drift monitor\n",
        "\n",
        "**Note:** This may take some time to run."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.widgets import RunDetails\n",
        "\n",
        "backfill = monitor.backfill(dt.datetime.now() - dt.timedelta(weeks=6), dt.datetime.now())\n",
        "\n",
        "RunDetails(backfill).show()\n",
        "backfill.wait_for_completion()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1621971219146
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze data drift"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "drift_metrics = backfill.get_metrics()\n",
        "for metric in drift_metrics:\n",
        "    print(metric, drift_metrics[metric])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1621971233332
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}