{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.4)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from nltk) (1.11.0)\n",
      "Requirement already satisfied: singledispatch in /usr/local/lib/python3.6/dist-packages (from nltk) (3.4.0.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.23.4)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.7.5)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.6/dist-packages (0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "!pip install ipython-autotime\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 197 ms\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 368 µs\n"
     ]
    }
   ],
   "source": [
    "title = \"20_newsgroups\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 52.5 ms\n"
     ]
    }
   ],
   "source": [
    "paths = []\n",
    "for (dirpath, dirnames, filenames) in os.walk(str(os.getcwd())+'/'+title+'/'):\n",
    "    for i in filenames:\n",
    "        paths.append(str(dirpath)+str(\"/\")+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/my_project_dir/20_newsgroups/talk.politics.guns/54541'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.94 ms\n"
     ]
    }
   ],
   "source": [
    "paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.4 ms\n"
     ]
    }
   ],
   "source": [
    "# paths.remove(paths[0]) #my computer is generating a dummy file, removing that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19997"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.36 ms\n"
     ]
    }
   ],
   "source": [
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.29 ms\n"
     ]
    }
   ],
   "source": [
    "def remove_header(data):\n",
    "    try:\n",
    "        ind = data.index('\\n\\n')\n",
    "        data = data[ind:]\n",
    "    except:\n",
    "        print(\"No Header\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 594 µs\n"
     ]
    }
   ],
   "source": [
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.31 ms\n"
     ]
    }
   ],
   "source": [
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return np.char.strip(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.51 ms\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 947 µs\n"
     ]
    }
   ],
   "source": [
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.42 ms\n"
     ]
    }
   ],
   "source": [
    "def remove_single_characters(data):\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return np.char.strip(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.63 ms\n"
     ]
    }
   ],
   "source": [
    "def convert_numbers(data):\n",
    "    data = np.char.replace(data, \"0\", \" zero \")\n",
    "    data = np.char.replace(data, \"1\", \" one \")\n",
    "    data = np.char.replace(data, \"2\", \" two \")\n",
    "    data = np.char.replace(data, \"3\", \" three \")\n",
    "    data = np.char.replace(data, \"4\", \" four \")\n",
    "    data = np.char.replace(data, \"5\", \" five \")\n",
    "    data = np.char.replace(data, \"6\", \" six \")\n",
    "    data = np.char.replace(data, \"7\", \" seven \")\n",
    "    data = np.char.replace(data, \"8\", \" eight \")\n",
    "    data = np.char.replace(data, \"9\", \" nine \")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.68 ms\n"
     ]
    }
   ],
   "source": [
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return np.char.strip(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.32 ms\n"
     ]
    }
   ],
   "source": [
    "def preprocess(data, query):\n",
    "    if not query:\n",
    "        data = remove_header(data)        \n",
    "    data = convert_lower_case(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = remove_stop_words(data)\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_single_characters(data)\n",
    "    data = stemming(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Postings for Unigram inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "No Header\n",
      "1300\n",
      "1400\n",
      "No Header\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "No Header\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "No Header\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "No Header\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "No Header\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "No Header\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "No Header\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "No Header\n",
      "7100\n",
      "No Header\n",
      "7200\n",
      "7300\n",
      "No Header\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "No Header\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "No Header\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "No Header\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "No Header\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "No Header\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "No Header\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "No Header\n",
      "14000\n",
      "14100\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "No Header\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "No Header\n",
      "14500\n",
      "14600\n",
      "No Header\n",
      "14700\n",
      "14800\n",
      "No Header\n",
      "No Header\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "No Header\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "No Header\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "No Header\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "No Header\n",
      "18200\n",
      "18300\n",
      "No Header\n",
      "No Header\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "No Header\n",
      "No Header\n",
      "19200\n",
      "No Header\n",
      "19300\n",
      "No Header\n",
      "19400\n",
      "19500\n",
      "No Header\n",
      "19600\n",
      "No Header\n",
      "19700\n",
      "No Header\n",
      "No Header\n",
      "19800\n",
      "No Header\n",
      "19900\n",
      "time: 32min 46s\n"
     ]
    }
   ],
   "source": [
    "doc = 0\n",
    "postings = pd.DataFrame()\n",
    "\n",
    "for path in paths:\n",
    "    file = open(path, 'r', encoding='cp1250')\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "    preprocessed_text = preprocess(text, False)\n",
    "    \n",
    "    if doc%100 == 0:\n",
    "        print(doc)\n",
    "\n",
    "    tokens = word_tokenize(str(preprocessed_text))\n",
    "    for token in tokens:\n",
    "        if token in postings:\n",
    "            p = postings[token][0]\n",
    "            p.add(doc)\n",
    "            postings[token][0] = p\n",
    "        else:\n",
    "            postings.insert(value=[{doc}], loc=0, column=token)\n",
    "    doc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>midweek</th>\n",
       "      <th>grafx</th>\n",
       "      <th>tasco</th>\n",
       "      <th>stoneham</th>\n",
       "      <th>montval</th>\n",
       "      <th>homewrit</th>\n",
       "      <th>jourdain</th>\n",
       "      <th>turley</th>\n",
       "      <th>dahmk</th>\n",
       "      <th>foosum</th>\n",
       "      <th>...</th>\n",
       "      <th>forev</th>\n",
       "      <th>wait</th>\n",
       "      <th>demand</th>\n",
       "      <th>nay</th>\n",
       "      <th>will</th>\n",
       "      <th>mani</th>\n",
       "      <th>fact</th>\n",
       "      <th>comment</th>\n",
       "      <th>notic</th>\n",
       "      <th>anyon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{19996}</td>\n",
       "      <td>{19994}</td>\n",
       "      <td>{19990}</td>\n",
       "      <td>{19989}</td>\n",
       "      <td>{19989}</td>\n",
       "      <td>{19985}</td>\n",
       "      <td>{19984}</td>\n",
       "      <td>{19984}</td>\n",
       "      <td>{19984}</td>\n",
       "      <td>{19984}</td>\n",
       "      <td>...</td>\n",
       "      <td>{0, 6150, 16904, 14, 3598, 10766, 10769, 536, ...</td>\n",
       "      <td>{0, 8194, 12291, 8200, 8203, 10252, 14, 14351,...</td>\n",
       "      <td>{0, 5121, 15361, 3, 11268, 15873, 5639, 15370,...</td>\n",
       "      <td>{0, 15241, 14, 16787, 15639, 4637, 3761, 1075,...</td>\n",
       "      <td>{0, 8194, 14341, 2054, 11, 4109, 14, 15, 12304...</td>\n",
       "      <td>{0, 8192, 4, 5, 6, 8196, 14, 17, 25, 8218, 27,...</td>\n",
       "      <td>{0, 1, 16391, 8, 14, 15, 17, 8209, 16404, 22, ...</td>\n",
       "      <td>{0, 8192, 14336, 10243, 10245, 12294, 2055, 16...</td>\n",
       "      <td>{0, 12291, 10246, 10, 4107, 2061, 14, 18445, 1...</td>\n",
       "      <td>{0, 16385, 4, 8, 16393, 16394, 11, 8203, 16397...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 90027 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   midweek    grafx    tasco stoneham  montval homewrit jourdain   turley  \\\n",
       "0  {19996}  {19994}  {19990}  {19989}  {19989}  {19985}  {19984}  {19984}   \n",
       "\n",
       "     dahmk   foosum                        ...                          \\\n",
       "0  {19984}  {19984}                        ...                           \n",
       "\n",
       "                                               forev  \\\n",
       "0  {0, 6150, 16904, 14, 3598, 10766, 10769, 536, ...   \n",
       "\n",
       "                                                wait  \\\n",
       "0  {0, 8194, 12291, 8200, 8203, 10252, 14, 14351,...   \n",
       "\n",
       "                                              demand  \\\n",
       "0  {0, 5121, 15361, 3, 11268, 15873, 5639, 15370,...   \n",
       "\n",
       "                                                 nay  \\\n",
       "0  {0, 15241, 14, 16787, 15639, 4637, 3761, 1075,...   \n",
       "\n",
       "                                                will  \\\n",
       "0  {0, 8194, 14341, 2054, 11, 4109, 14, 15, 12304...   \n",
       "\n",
       "                                                mani  \\\n",
       "0  {0, 8192, 4, 5, 6, 8196, 14, 17, 25, 8218, 27,...   \n",
       "\n",
       "                                                fact  \\\n",
       "0  {0, 1, 16391, 8, 14, 15, 17, 8209, 16404, 22, ...   \n",
       "\n",
       "                                             comment  \\\n",
       "0  {0, 8192, 14336, 10243, 10245, 12294, 2055, 16...   \n",
       "\n",
       "                                               notic  \\\n",
       "0  {0, 12291, 10246, 10, 4107, 2061, 14, 18445, 1...   \n",
       "\n",
       "                                               anyon  \n",
       "0  {0, 16385, 4, 8, 16393, 16394, 11, 8203, 16397...  \n",
       "\n",
       "[1 rows x 90027 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 35.8 ms\n"
     ]
    }
   ],
   "source": [
    "postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 179 ms\n"
     ]
    }
   ],
   "source": [
    "postings.to_pickle(title + \"_unigram_postings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 506 ms\n"
     ]
    }
   ],
   "source": [
    "postings = pd.read_pickle(title + \"_unigram_postings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (complex)Search Query for Unigram Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 827 µs\n"
     ]
    }
   ],
   "source": [
    "def get_word_postings(word):\n",
    "    preprocessed_word = str(preprocess(word, True))\n",
    "    print(preprocessed_word)\n",
    "    print(\"Frequency:\",len(postings[preprocessed_word][0]))\n",
    "    print(\"Postings List:\",postings[preprocessed_word][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 971 µs\n"
     ]
    }
   ],
   "source": [
    "def get_not(word):\n",
    "    a = postings[word][0]\n",
    "    b = set(range(len(paths)))\n",
    "    return b.difference(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.21 ms\n"
     ]
    }
   ],
   "source": [
    "def generate_command_tokens(query):\n",
    "    query = query.lower()\n",
    "    tokens = word_tokenize(query)\n",
    "\n",
    "    commands = []\n",
    "    query_words = []\n",
    "\n",
    "    for t in tokens:\n",
    "        if t not in ['and', 'or', 'not']:\n",
    "            processed_word = preprocess([t], True)\n",
    "            print(str(processed_word))\n",
    "            query_words.append(str(processed_word))\n",
    "        else:\n",
    "            commands.append(t)\n",
    "    \n",
    "    return commands, query_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.26 ms\n"
     ]
    }
   ],
   "source": [
    "def gen_not_tuple(query_words, commands):\n",
    "    tup = []\n",
    "    while 'not' in commands:\n",
    "        i = commands.index('not')\n",
    "        word = query_words[i]\n",
    "        word_postings = get_not(word)\n",
    "        tup.append(word_postings)\n",
    "        commands.pop(i)\n",
    "        query_words[i] = i\n",
    "        print(\"\\nAfter Not Processing:\",commands, query_words)\n",
    "    return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.46 ms\n"
     ]
    }
   ],
   "source": [
    "def binary_operations(query_words, commands, tup):\n",
    "    a = postings[query_words[0]][0]\n",
    "    query_words.pop(0)\n",
    "\n",
    "    for i in range(len(commands)):\n",
    "        if type(query_words[i])==int:\n",
    "            b = tup.pop(0)\n",
    "        else:\n",
    "            b = postings[query_words[i]][0]\n",
    "\n",
    "        if commands[i] == 'and':\n",
    "            a = a.intersection(b)\n",
    "        elif commands[i] == 'or':\n",
    "            a = a.union(b)\n",
    "        else:\n",
    "            print(\"Invalid Command\")\n",
    "            \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.12 ms\n"
     ]
    }
   ],
   "source": [
    "def execute_query(query):\n",
    "\n",
    "    commands, query_words = generate_command_tokens(query)\n",
    "    tup = gen_not_tuple(query_words, commands)\n",
    "    \n",
    "    print(\"\\nCommands:\",commands)\n",
    "    print(\"\\nQuery Words:\",query_words)\n",
    "    print(\"\\nTup:\",len(tup))\n",
    "    \n",
    "    final_set = binary_operations(query_words, commands, tup)\n",
    "    \n",
    "    print(\"\\nFinal Set:\",final_set)\n",
    "    return final_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.09 ms\n"
     ]
    }
   ],
   "source": [
    "def print_file(file):\n",
    "    out_file = open(paths[file], 'r', encoding='cp1250')\n",
    "    out_text = out_file.read()\n",
    "    print(out_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.05 ms\n"
     ]
    }
   ],
   "source": [
    "query = \"place and inhumane and not authenticity and not hell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "place\n",
      "inhuman\n",
      "authent\n",
      "hell\n",
      "\n",
      "After Not Processing: ['and', 'and', 'and', 'not'] ['place', 'inhuman', 2, 'hell']\n",
      "\n",
      "After Not Processing: ['and', 'and', 'and'] ['place', 'inhuman', 2, 3]\n",
      "\n",
      "Commands: ['and', 'and', 'and']\n",
      "\n",
      "Query Words: ['place', 'inhuman', 2, 3]\n",
      "\n",
      "Tup: 2\n",
      "\n",
      "Final Set: {5349, 5131, 845, 5550, 5331, 5881, 5463, 5624, 5817, 5438}\n",
      "time: 30.2 ms\n"
     ]
    }
   ],
   "source": [
    "lists = execute_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "welcom\n",
      "Frequency: 314\n",
      "Postings List: {9216, 17925, 1032, 15371, 17421, 8718, 7695, 17422, 5649, 16917, 4119, 4122, 14876, 10781, 18974, 5156, 11815, 9768, 18473, 7722, 9259, 4652, 15406, 3631, 17456, 561, 3633, 6195, 6708, 7217, 7731, 572, 9789, 14908, 13887, 14912, 19521, 6211, 5700, 12867, 13385, 11338, 15949, 2130, 4179, 596, 17490, 11862, 14938, 92, 10846, 2143, 17504, 609, 10851, 103, 11879, 2153, 17001, 18027, 624, 1137, 7797, 17525, 13432, 17530, 4730, 17532, 17021, 1661, 13438, 4737, 6273, 13953, 14979, 15491, 9350, 11260, 2184, 3208, 5770, 17547, 3720, 11918, 16527, 9362, 13975, 16537, 9882, 667, 11932, 16541, 17565, 13983, 19099, 11427, 8868, 13477, 9382, 5288, 3755, 12459, 17073, 18609, 5815, 7351, 18617, 18619, 10941, 13501, 19646, 16576, 5316, 6340, 16581, 11978, 16074, 14032, 11985, 14034, 11475, 10452, 15569, 13526, 15062, 729, 10459, 3804, 11484, 15070, 16607, 8928, 19169, 17123, 8933, 13541, 10471, 17126, 13545, 19176, 8427, 11500, 13035, 10992, 14064, 15605, 19189, 3319, 7415, 14073, 4346, 19193, 253, 6398, 6910, 11519, 11521, 18688, 15621, 17158, 5895, 7944, 15623, 267, 2828, 8978, 11030, 18198, 12056, 5402, 5915, 3868, 7453, 10011, 13600, 17697, 5922, 14114, 1318, 5926, 3880, 2857, 6950, 9510, 17191, 17194, 17199, 15153, 4915, 2868, 11060, 15156, 1847, 15668, 2874, 4922, 15674, 1346, 5442, 11588, 2373, 17730, 17737, 843, 8524, 9036, 13646, 19276, 17744, 337, 338, 7506, 11602, 13138, 18262, 19280, 17240, 15195, 10078, 8544, 11617, 2402, 14177, 13668, 17248, 19300, 13164, 1389, 6511, 1904, 15221, 17270, 8568, 8573, 11646, 10111, 6528, 10623, 13183, 17277, 17282, 14214, 13703, 2952, 9608, 12170, 14218, 2444, 10124, 14726, 18311, 18317, 16273, 13202, 17814, 1943, 7065, 6554, 8089, 2973, 17827, 13220, 17319, 17320, 937, 6058, 15273, 1965, 17838, 15792, 13746, 15283, 15796, 7605, 3510, 5562, 11708, 17341, 2494, 19391, 19904, 962, 14275, 15299, 16837, 3016, 14281, 18376, 18888, 15824, 9682, 18901, 15830, 17880, 17374, 481, 5089, 18916, 17381, 19940, 10727, 17384, 5611, 16363, 17388, 18413, 19951, 16880, 11763, 17908, 17397, 13820, 4093, 6142}\n",
      "time: 3.11 ms\n"
     ]
    }
   ],
   "source": [
    "get_word_postings('welcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newsgroups: talk.politics.guns\n",
      "Path: cantaloupe.srv.cs.cmu.edu!magnesium.club.cc.cmu.edu!news.sei.cmu.edu!cis.ohio-state.edu!zaphod.mps.ohio-state.edu!darwin.sura.net!paladin.american.edu!europa.eng.gtefsd.com!emory!rsiatl!karl\n",
      "From: karl@dixie.com (Karl Klingman)\n",
      "Subject: Re: The Truth about Waco \n",
      "Message-ID: <_+tvn0k@dixie.com>\n",
      "Date: Mon, 26 Apr 93 19:40:52 GMT\n",
      "Organization: Dixie Communications Public Access.  The Mouth of the South.\n",
      "Distribution: usa\n",
      "References: <25337@alice.att.com> <1ppv94-@dixie.com> <C6054E.BJq@chinet.chi.il.us>\n",
      "Lines: 74\n",
      "\n",
      "dhartung@chinet.chi.il.us (Dan Hartung) writes:\n",
      "\n",
      ">jgd@dixie.com (John De Armond) writes:\n",
      "\n",
      ">>*\tThe tanks were collapsing interior walls and ceilings putting people\n",
      ">>\tat great risk.\n",
      "\n",
      ">Dear, dear. They could have COME OUT.\n",
      "\n",
      "Then by your logic, the Jews in Europe in the 1930's were the cause for\n",
      "the Holocaust.  Hitler told them to leave and because they didn't they\n",
      "brought the whole thing on themselves.  Because as you say, they could\n",
      "have COME OUT of Germany.\n",
      "\n",
      ">>*\tThere was no group instruction of any kind from Koresh or his \n",
      ">>\taids after the tank invasion (referring to any kind of suicide\n",
      ">>\tpact or counter-assault efforts.)\n",
      "\n",
      "\n",
      ">It's ultimately irrelevant who \"lit\" the fire.  They had ample opportunity\n",
      ">to LEAVE.\n",
      "\n",
      "Same for the Jews in Europe 1930's.\n",
      "\n",
      ">While he was there.  Anyway, outsiders RARELY see abuse.  It's a secretive\n",
      ">thing.  All we have to go on are the court documents in the Jewell case\n",
      ">and the mistrial in California.\n",
      "\n",
      "You don't see any evidence of the abuse -- therefore it must be taking place?\n",
      "As you point out everwhere but here, it is irrelevant to this case.  The\n",
      "ATF is not in charge of investigating child abuse.\n",
      "\n",
      ">>*\tNo one was ever held against their wills and could have left at any\n",
      ">>\ttime.  The people who were murdered in the fire were there by their\n",
      ">>\town choices.\n",
      "\n",
      ">EXACTLY.  By their OWN CHOICE.\n",
      "\n",
      "In obvious contradiction to the statements made by the F. B. I.\n",
      "\n",
      ">I have NEVER judged them by their religion, but by their ACTIONS.\n",
      "\n",
      "And just what are those actions that you are judging them by?\n",
      "Their refusal to let the government control their lives? Their refusal\n",
      "to submit to unconstitutional laws?  Their refusal to behave like\n",
      "cowards?  Some of Texas' heros could have taken the cowardly way\n",
      "out too and surrendered the Alamo.  After all, all they had to do was\n",
      "COME OUT.  They stayed as you say by their \"OWN CHOICE\".  Problem\n",
      "is not everyone chooses to act like a groveling dog in the face of\n",
      "insurmountable odds.  But as you point out, they certainly do have\n",
      "that right.  \n",
      "\n",
      ">If they had lived a quiet, religious life as they claimed, there would\n",
      ">have been no raid, no siege, and no deaths.  Instead, they chose courses\n",
      ">of action at every turn that were at the very least STUPID, if not\n",
      ">IRRATIONAL.  The first was to stockpile weapons.  The second was to\n",
      ">shoot federal agents.  The third was to stay inside.\n",
      "\n",
      "Bull.  They did, in fact, live a quiet, religious life -- as they claimed.\n",
      "The warrant was not issued because they \"stockpiled weapons\".  It is\n",
      "not against the law to own as many guns as you want -- yet (Except in \n",
      "Virginia).The warrant was issued for some \"gun parts\" that are about the size \n",
      "of a half-dollar.  Certainly worth the lives of so many people, don't you \n",
      "think?\n",
      "\n",
      ">Just as we don't blame a cop who shoots a kid who had pointed a toy\n",
      ">weapon at him, I don't think the FBI deserves blame in this case.\n",
      "\n",
      "You can forget that WE business.  I certainly do blame them.\n",
      "\n",
      "-- \n",
      "He who would trade his liberty for  |  Karl Klingman\n",
      "security deserves neither.          |  American Research Group, Inc.\n",
      "                                    |  karl@dixie.com\n",
      "\n",
      "time: 2.58 ms\n"
     ]
    }
   ],
   "source": [
    "print_file(84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
